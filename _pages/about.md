---
permalink: /
title: ""
layout: minimal
author_profile: false
---

<img src="/images/profile.jpg" alt="Dan Lee" class="profile-image">

# Dan Lee

Research Collaborator, MIT CSAIL | Former Growth Investor

<div class="social-links">
<a href="mailto:dani.cmh.lee@gmail.com"><i class="fas fa-envelope"></i> Email</a>
<a href="https://scholar.google.com/citations?user=9sXW7koAAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i> Scholar</a>
<a href="https://x.com/Danicmhlee" target="_blank"><i class="fab fa-twitter"></i> X</a>
<a href="https://www.linkedin.com/in/dan-l-92a06a135/" target="_blank"><i class="fab fa-linkedin"></i> LinkedIn</a>
<a href="https://github.com/danihyunlee" target="_blank"><i class="fab fa-github"></i> GitHub</a>
</div>

---

I'm interested in developing methods for learning general intelligence: interpretable representations and training procedures that transfer across diverse domains. My research focuses on synthetic data pretraining and alternative training paradigms beyond next-token prediction, with the goal of building models that acquire reusable structure rather than brittle, domain-specific patterns.

I'm currently a research collaborator at MIT CSAIL's Improbable AI Lab with Professor Pulkit Agrawal, where I study how synthetic training distributions—such as algorithmic and cellular-automata-based data—can induce useful inductive biases for reasoning, generalization, and data efficiency. My work explores how the structure and complexity of synthetic data affect downstream performance and transfer.

---

## Research Interests

- **Synthetic pretraining** — Using algorithmic and synthetic environments to study and improve inductive bias, data efficiency, and generalization
- **Training paradigms** — Exploring objectives and learning setups beyond next-token prediction to better support reasoning and abstraction
- **Transfer & representations** — Understanding what properties of training data and objectives enable robust cross-domain transfer
- **Structure & complexity** — Studying how the computational structure and complexity of pretraining data shape learned representations

---

## Background

Before focusing full-time on research, I worked as a growth investor at Bregal Sagemount and as an investment banker in Evercore's Technology M&A group. Working closely with software and ML-driven companies highlighted a recurring gap: modern models often perform well in narrow settings but struggle to adapt across contexts. That experience motivated my interest in more fundamental approaches to learning general, transferable representations.

I graduated *summa cum laude* from Columbia University with a B.S. in Computer Science (Intelligent Systems & ML Track) and a minor in Economics, where I was an Egleston Scholar. At Columbia, I focused on machine learning, robotics learning, and algorithms, and served as Head TA for Data Structures (COMS 3134).

---

## Beyond Research

Outside of research, I enjoy hiking, cooking, photography, reading, working out, and playing TFT. I'm particularly drawn to literature that explores identity, abstraction, and meaning—with favorites including Haruki Murakami, Kurt Vonnegut, and Hermann Hesse.
