---
permalink: /
title: ""
layout: minimal
author_profile: false
---

<img src="/images/profile.HEIC" alt="Dan Lee" class="profile-image">

# Dan Lee

Research Collaborator, MIT CSAIL | Former Growth Investor

<div class="social-links">
<a href="mailto:dani.cmh.lee@gmail.com"><i class="fas fa-envelope"></i> Email</a>
<a href="https://scholar.google.com/citations?user=9sXW7koAAAAJ&hl=en" target="_blank"><i class="fas fa-graduation-cap"></i> Scholar</a>
<a href="https://x.com/Danicmhlee" target="_blank"><i class="fab fa-twitter"></i> X</a>
<a href="https://www.linkedin.com/in/dan-l-92a06a135/" target="_blank"><i class="fab fa-linkedin"></i> LinkedIn</a>
<a href="https://github.com/danihyunlee" target="_blank"><i class="fab fa-github"></i> GitHub</a>
</div>

---

I'm interested in developing methods for learning general intelligence: interpretable representations and training procedures that transfer across diverse domains. My recent research focuses on synthetic data pretraining and alternative training paradigms beyond next-token prediction, with the goal of building models that acquire reusable structure rather than brittle, domain-specific patterns.

I'm currently a research collaborator at MIT CSAIL's Improbable AI Lab with Professor Pulkit Agrawal, where I study how synthetic training distributions — such as algorithmic and cellular-automata-based data — can induce useful inductive biases for reasoning, generalization, and data efficiency. My work explores how the structure and complexity of synthetic data affect downstream performance and transfer.

---

## Research Interests

- **Synthetic pretraining** — Using algorithmic and synthetic environments to study and improve inductive bias, data efficiency, and generalization
- **Training & Inference paradigms** — Exploring objectives and learning setups (test-time, continual, and reward-based learning) to better support reasoning and abstraction
- **Transfer & representations** — Understanding what properties of training data and objectives enable robust cross-domain transfer
- **Structure & complexity** — Studying how the computational structure and complexity of data shape learned representations

---

## Background

Before research, I worked as a growth equity investor at Bregal Sagemount and as an investment banker in Evercore's Technology M&A group focused on AI, software, and robotics. Working closely with leading software and ML-driven companies highlighted the gap between model capabilities and robust general reasoning suitable for real-world applications - motivating my current research interests.

I graduated *summa cum laude* from Columbia University with a B.S. in Computer Science (Intelligent Systems & ML Track) and a minor in Economics, where I was an Egleston Scholar. At Columbia, I focused on machine learning, robotics learning, and algorithms, and also served as Head TA for Data Structures (COMS 3134).

---

## Beyond Research

Outside of research, I enjoy hiking, cooking, photography, reading, working out, and playing TFT. I'm particularly drawn to literature that explores identity, abstraction, and meaning—with favorites including Haruki Murakami, Kurt Vonnegut, and Hermann Hesse.

I keep track of an active
[list](https://docs.google.com/document/d/1i3iEEdWL3dEMC0mdjl4SHEJrZDeSZ2Ns_NLoab1OmHY/edit?usp=sharing)
of quotes, articles, art, and books that inspire me.

Please feel free to email
me at dani.cmh.lee@gmail.com to chat or discuss.
